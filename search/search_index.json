{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Bub - Bub it. Build it. \u00b6 Bub is an AI-powered CLI tool that helps you build, develop, and manage projects using natural language commands. With access to file operations, command execution, and intelligent reasoning, Bub acts as your coding assistant. Quick Start \u00b6 Installation \u00b6 # Install from PyPI (when available) pip install bub # Or install from source git clone https://github.com/psiace/bub.git cd bub uv sync uv run bub --help Setup \u00b6 Configure your AI provider and model: # For OpenAI export BUB_PROVIDER = \"openai\" export BUB_MODEL_NAME = \"gpt-4\" export BUB_API_KEY = \"sk-...\" # For Anthropic export BUB_PROVIDER = \"anthropic\" export BUB_MODEL_NAME = \"claude-3-5-sonnet-20241022\" export BUB_API_KEY = \"your-anthropic-key\" # For local models with Ollama export BUB_PROVIDER = \"ollama\" export BUB_MODEL_NAME = \"llama3\" # No API key needed for local models # For Groq (fast inference) export BUB_PROVIDER = \"groq\" export BUB_MODEL_NAME = \"llama3-8b-8192\" export BUB_API_KEY = \"gsk_...\" Usage \u00b6 # Interactive chat mode bub chat # Run a single command bub run \"Create a Python script that prints 'Hello, World!'\" # Specify workspace and model (provider/model format) bub chat --workspace /path/to/project --model openai/gpt-4 # Get help bub --help Examples \u00b6 Here are some ways you can use Bub: File Creation bub run \"Create a README.md for a Python project\" Code Assistance bub run \"Add error handling to my main.py file\" Project Setup bub run \"Initialize a new FastAPI project with basic structure\" Code Review bub run \"Review my code and suggest improvements\" Configuration \u00b6 Configure Bub via environment variables or a .env file: Variable Description Example BUB_PROVIDER AI provider name (required) openai , anthropic , ollama BUB_MODEL_NAME Model name from provider (required) gpt-4 , claude-3-5-sonnet-20241022 BUB_API_KEY API key for provider (not needed for local models) sk-... BUB_API_BASE Custom API endpoint (optional) https://api.custom.ai BUB_MAX_TOKENS Maximum response tokens (optional) 4000 BUB_WORKSPACE_PATH Default workspace directory (optional) /path/to/work BUB_SYSTEM_PROMPT Custom system prompt (optional) \"You are a helpful assistant...\" Custom System Prompt with BUB.md \u00b6 You can customize Bub's behavior by creating a BUB.md file in your workspace. This file will be automatically read and used as the system prompt, allowing you to define project-specific instructions, coding standards, and behavior guidelines. Example BUB.md: # Project Assistant You are a Python development assistant for this specific project. ## Guidelines - Follow PEP 8 style guidelines - Use type hints for all functions - Write comprehensive tests with pytest - Focus on security and performance ## Project Structure - `/src` - Main source code - `/tests` - Test files - `/docs` - Documentation When making changes, always run tests first. The BUB.md file takes precedence over the BUB_SYSTEM_PROMPT environment variable, making it easy to share consistent AI behavior across your development team. Supported Providers \u00b6 Bub supports all providers available through Any-LLM : OpenAI : openai - GPT-4, GPT-3.5-turbo, etc. Anthropic : anthropic - Claude models Ollama : ollama - Local models (Llama, CodeLlama, etc.) Groq : groq - Fast inference for open models Mistral : mistral - Mistral AI models Cohere : cohere - Cohere models Google : google - Gemini models And many more... Commands \u00b6 bub chat \u00b6 Start an interactive chat session with Bub. Options: - --workspace PATH : Set the workspace directory - --model MODEL : Specify the AI model to use - --max-tokens INT : Set maximum response tokens Interactive Commands: - quit , exit , q : End the session - reset : Clear conversation history - debug : Toggle debug mode to see AI reasoning process bub run \u00b6 Execute a single command with Bub. Usage: bub run \"COMMAND\" [ OPTIONS ] Options: - --workspace PATH : Set the workspace directory - --model MODEL : Specify the AI model to use - --max-tokens INT : Set maximum response tokens Links \u00b6 GitHub Repository : https://github.com/psiace/bub/ Issues & Bug Reports : https://github.com/psiace/bub/issues Contributing : https://github.com/psiace/bub/blob/main/CONTRIBUTING.md","title":"Home"},{"location":"#bub-bub-it-build-it","text":"Bub is an AI-powered CLI tool that helps you build, develop, and manage projects using natural language commands. With access to file operations, command execution, and intelligent reasoning, Bub acts as your coding assistant.","title":"Bub - Bub it. Build it."},{"location":"#quick-start","text":"","title":"Quick Start"},{"location":"#installation","text":"# Install from PyPI (when available) pip install bub # Or install from source git clone https://github.com/psiace/bub.git cd bub uv sync uv run bub --help","title":"Installation"},{"location":"#setup","text":"Configure your AI provider and model: # For OpenAI export BUB_PROVIDER = \"openai\" export BUB_MODEL_NAME = \"gpt-4\" export BUB_API_KEY = \"sk-...\" # For Anthropic export BUB_PROVIDER = \"anthropic\" export BUB_MODEL_NAME = \"claude-3-5-sonnet-20241022\" export BUB_API_KEY = \"your-anthropic-key\" # For local models with Ollama export BUB_PROVIDER = \"ollama\" export BUB_MODEL_NAME = \"llama3\" # No API key needed for local models # For Groq (fast inference) export BUB_PROVIDER = \"groq\" export BUB_MODEL_NAME = \"llama3-8b-8192\" export BUB_API_KEY = \"gsk_...\"","title":"Setup"},{"location":"#usage","text":"# Interactive chat mode bub chat # Run a single command bub run \"Create a Python script that prints 'Hello, World!'\" # Specify workspace and model (provider/model format) bub chat --workspace /path/to/project --model openai/gpt-4 # Get help bub --help","title":"Usage"},{"location":"#examples","text":"Here are some ways you can use Bub: File Creation bub run \"Create a README.md for a Python project\" Code Assistance bub run \"Add error handling to my main.py file\" Project Setup bub run \"Initialize a new FastAPI project with basic structure\" Code Review bub run \"Review my code and suggest improvements\"","title":"Examples"},{"location":"#configuration","text":"Configure Bub via environment variables or a .env file: Variable Description Example BUB_PROVIDER AI provider name (required) openai , anthropic , ollama BUB_MODEL_NAME Model name from provider (required) gpt-4 , claude-3-5-sonnet-20241022 BUB_API_KEY API key for provider (not needed for local models) sk-... BUB_API_BASE Custom API endpoint (optional) https://api.custom.ai BUB_MAX_TOKENS Maximum response tokens (optional) 4000 BUB_WORKSPACE_PATH Default workspace directory (optional) /path/to/work BUB_SYSTEM_PROMPT Custom system prompt (optional) \"You are a helpful assistant...\"","title":"Configuration"},{"location":"#custom-system-prompt-with-bubmd","text":"You can customize Bub's behavior by creating a BUB.md file in your workspace. This file will be automatically read and used as the system prompt, allowing you to define project-specific instructions, coding standards, and behavior guidelines. Example BUB.md: # Project Assistant You are a Python development assistant for this specific project. ## Guidelines - Follow PEP 8 style guidelines - Use type hints for all functions - Write comprehensive tests with pytest - Focus on security and performance ## Project Structure - `/src` - Main source code - `/tests` - Test files - `/docs` - Documentation When making changes, always run tests first. The BUB.md file takes precedence over the BUB_SYSTEM_PROMPT environment variable, making it easy to share consistent AI behavior across your development team.","title":"Custom System Prompt with BUB.md"},{"location":"#supported-providers","text":"Bub supports all providers available through Any-LLM : OpenAI : openai - GPT-4, GPT-3.5-turbo, etc. Anthropic : anthropic - Claude models Ollama : ollama - Local models (Llama, CodeLlama, etc.) Groq : groq - Fast inference for open models Mistral : mistral - Mistral AI models Cohere : cohere - Cohere models Google : google - Gemini models And many more...","title":"Supported Providers"},{"location":"#commands","text":"","title":"Commands"},{"location":"#bub-chat","text":"Start an interactive chat session with Bub. Options: - --workspace PATH : Set the workspace directory - --model MODEL : Specify the AI model to use - --max-tokens INT : Set maximum response tokens Interactive Commands: - quit , exit , q : End the session - reset : Clear conversation history - debug : Toggle debug mode to see AI reasoning process","title":"bub chat"},{"location":"#bub-run","text":"Execute a single command with Bub. Usage: bub run \"COMMAND\" [ OPTIONS ] Options: - --workspace PATH : Set the workspace directory - --model MODEL : Specify the AI model to use - --max-tokens INT : Set maximum response tokens","title":"bub run"},{"location":"#links","text":"GitHub Repository : https://github.com/psiace/bub/ Issues & Bug Reports : https://github.com/psiace/bub/issues Contributing : https://github.com/psiace/bub/blob/main/CONTRIBUTING.md","title":"Links"},{"location":"posts/2025-07-16-baby-bub-bootstrap-milestone/","text":"Baby Bub: From Inspiration to Bootstrap Milestone \u00b6 Genesis: Inspiration from Modern Agents \u00b6 Bub is a CLI-first AI agent, built to \"Bub it. Build it.\" The project draws direct inspiration from How to Build an Agent and Tiny Agents: Building LLM-Powered Agents from Scratch . Both resources distill the essence of tool-using, loop-based, composable, and extensible agents. But Bub is also a response to the new wave of self-improving, self-hosting agents: think Claude Code, SWE-agent, and the broader \"self-bootstrapping\" movement. The goal: an agent that can not only help you build, but can help build (and fix) itself. Architecture: ReAct Loop, Tools, and CLI \u00b6 The ReAct Loop \u00b6 At the heart of Bub is a classic ReAct loop, implemented in src/bub/agent/core.py : class Agent : ... def chat ( self , message : str , on_step : Optional [ Callable [[ str , str ], None ]] = None ) -> str : self . conversation_history . append ( Message ( role = \"user\" , content = message )) while True : ... response = litellm . completion ( ... ) assistant_message = str ( response . choices [ 0 ] . message . content ) self . conversation_history . append ( Message ( role = \"assistant\" , content = assistant_message )) ... tool_calls = self . tool_executor . extract_tool_calls ( assistant_message ) if tool_calls : for tool_call in tool_calls : ... result = self . tool_executor . execute_tool ( tool_name , ** parameters ) observation = f \"Observation: { result . format_result () } \" self . conversation_history . append ( Message ( role = \"user\" , content = observation )) ... continue else : return assistant_message This loop enables the agent to: Parse LLM output for tool calls (ReAct pattern: Thought, Action, Action Input, Observation). Execute tools (file read/write/edit, shell commands) and feed results back into the conversation. Iterate until a \"Final Answer\" is produced. Tool System: Extensible and Safe \u00b6 Tools are registered via a ToolRegistry ( src/bub/agent/tools.py ), and each tool is a Pydantic model with validation and metadata. For example, the RunCommandTool blocks dangerous commands and validates input: class RunCommandTool ( Tool ): ... DANGEROUS_COMMANDS : ClassVar [ set [ str ]] = { \"rm\" , \"del\" , ... } def _validate_command ( self ) -> Optional [ str ]: ... if base_cmd in self . DANGEROUS_COMMANDS : return f \"Dangerous command blocked: { base_cmd } \" This design makes it possible for the agent to safely self-modify, run tests, or even edit its own codebase\u2014crucial for self-improvement. CLI: User Experience and Debuggability \u00b6 The CLI ( src/bub/cli/app.py ) is built with Typer and Rich, providing a modern, user-friendly interface. The renderer ( src/bub/cli/render.py ) supports debug toggling, minimal/verbose TAAO (Thought/Action/Action Input/Observation) output, and clear error reporting. class Renderer : def __init__ ( self ) -> None : self . console : Console = Console () self . _show_debug : bool = False ... Milestone: The First mypy Fix (and Why It Matters) \u00b6 Bub aspires to self-improvement. The first tangible milestone? Fixing the very first mypy error: adding a missing return type annotation to Renderer.__init__ , check out the commit . - def __init__(self): - self.console = Console() - self._show_debug = False + def __init__(self) -> None: + self.console: Console = Console() + self._show_debug: bool = False This change reduced the mypy error count from 24 to 23. Trivial? Maybe. But it's a proof of concept: the agent can reason about, locate, and fix type errors in its own codebase. This is the first step toward a self-hosting, self-healing agent loop\u2014one that can eventually: Run static analysis on itself Propose and apply code fixes Test and validate improvements Looking Forward: Bub as a Bootstrap Agent \u00b6 Bub is still early. But the architecture is in place for: LLM-driven code editing and refactoring Automated type and lint fixes CLI-driven, user-friendly agent workflows The journey from \"fixing a mypy annotation\" to \"full agent self-improvement\" is long, but every bootstrap starts with a single, type-safe step. Project on GitHub Inspired by ampcode.com/how-to-build-an-agent and huggingface.co/blog/tiny-agents See also: Claude Code, SWE-agent, and the broader self-bootstrapping movement","title":"Baby Bub - Bootstrap Milestone"},{"location":"posts/2025-07-16-baby-bub-bootstrap-milestone/#baby-bub-from-inspiration-to-bootstrap-milestone","text":"","title":"Baby Bub: From Inspiration to Bootstrap Milestone"},{"location":"posts/2025-07-16-baby-bub-bootstrap-milestone/#genesis-inspiration-from-modern-agents","text":"Bub is a CLI-first AI agent, built to \"Bub it. Build it.\" The project draws direct inspiration from How to Build an Agent and Tiny Agents: Building LLM-Powered Agents from Scratch . Both resources distill the essence of tool-using, loop-based, composable, and extensible agents. But Bub is also a response to the new wave of self-improving, self-hosting agents: think Claude Code, SWE-agent, and the broader \"self-bootstrapping\" movement. The goal: an agent that can not only help you build, but can help build (and fix) itself.","title":"Genesis: Inspiration from Modern Agents"},{"location":"posts/2025-07-16-baby-bub-bootstrap-milestone/#architecture-react-loop-tools-and-cli","text":"","title":"Architecture: ReAct Loop, Tools, and CLI"},{"location":"posts/2025-07-16-baby-bub-bootstrap-milestone/#the-react-loop","text":"At the heart of Bub is a classic ReAct loop, implemented in src/bub/agent/core.py : class Agent : ... def chat ( self , message : str , on_step : Optional [ Callable [[ str , str ], None ]] = None ) -> str : self . conversation_history . append ( Message ( role = \"user\" , content = message )) while True : ... response = litellm . completion ( ... ) assistant_message = str ( response . choices [ 0 ] . message . content ) self . conversation_history . append ( Message ( role = \"assistant\" , content = assistant_message )) ... tool_calls = self . tool_executor . extract_tool_calls ( assistant_message ) if tool_calls : for tool_call in tool_calls : ... result = self . tool_executor . execute_tool ( tool_name , ** parameters ) observation = f \"Observation: { result . format_result () } \" self . conversation_history . append ( Message ( role = \"user\" , content = observation )) ... continue else : return assistant_message This loop enables the agent to: Parse LLM output for tool calls (ReAct pattern: Thought, Action, Action Input, Observation). Execute tools (file read/write/edit, shell commands) and feed results back into the conversation. Iterate until a \"Final Answer\" is produced.","title":"The ReAct Loop"},{"location":"posts/2025-07-16-baby-bub-bootstrap-milestone/#tool-system-extensible-and-safe","text":"Tools are registered via a ToolRegistry ( src/bub/agent/tools.py ), and each tool is a Pydantic model with validation and metadata. For example, the RunCommandTool blocks dangerous commands and validates input: class RunCommandTool ( Tool ): ... DANGEROUS_COMMANDS : ClassVar [ set [ str ]] = { \"rm\" , \"del\" , ... } def _validate_command ( self ) -> Optional [ str ]: ... if base_cmd in self . DANGEROUS_COMMANDS : return f \"Dangerous command blocked: { base_cmd } \" This design makes it possible for the agent to safely self-modify, run tests, or even edit its own codebase\u2014crucial for self-improvement.","title":"Tool System: Extensible and Safe"},{"location":"posts/2025-07-16-baby-bub-bootstrap-milestone/#cli-user-experience-and-debuggability","text":"The CLI ( src/bub/cli/app.py ) is built with Typer and Rich, providing a modern, user-friendly interface. The renderer ( src/bub/cli/render.py ) supports debug toggling, minimal/verbose TAAO (Thought/Action/Action Input/Observation) output, and clear error reporting. class Renderer : def __init__ ( self ) -> None : self . console : Console = Console () self . _show_debug : bool = False ...","title":"CLI: User Experience and Debuggability"},{"location":"posts/2025-07-16-baby-bub-bootstrap-milestone/#milestone-the-first-mypy-fix-and-why-it-matters","text":"Bub aspires to self-improvement. The first tangible milestone? Fixing the very first mypy error: adding a missing return type annotation to Renderer.__init__ , check out the commit . - def __init__(self): - self.console = Console() - self._show_debug = False + def __init__(self) -> None: + self.console: Console = Console() + self._show_debug: bool = False This change reduced the mypy error count from 24 to 23. Trivial? Maybe. But it's a proof of concept: the agent can reason about, locate, and fix type errors in its own codebase. This is the first step toward a self-hosting, self-healing agent loop\u2014one that can eventually: Run static analysis on itself Propose and apply code fixes Test and validate improvements","title":"Milestone: The First mypy Fix (and Why It Matters)"},{"location":"posts/2025-07-16-baby-bub-bootstrap-milestone/#looking-forward-bub-as-a-bootstrap-agent","text":"Bub is still early. But the architecture is in place for: LLM-driven code editing and refactoring Automated type and lint fixes CLI-driven, user-friendly agent workflows The journey from \"fixing a mypy annotation\" to \"full agent self-improvement\" is long, but every bootstrap starts with a single, type-safe step. Project on GitHub Inspired by ampcode.com/how-to-build-an-agent and huggingface.co/blog/tiny-agents See also: Claude Code, SWE-agent, and the broader self-bootstrapping movement","title":"Looking Forward: Bub as a Bootstrap Agent"}]}