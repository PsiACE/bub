# Bub Configuration
# Copy this file to .env and update with your settings

# Model configuration (Required)
# Format: provider:model
# Examples:
# - OpenAI: openai:gpt-4o-mini
# - Anthropic: anthropic:claude-3-5-sonnet-20241022
# - Ollama: ollama:llama3
# - Groq: groq:llama3-8b-8192
# - Mistral: mistral:mistral-large-latest
BUB_MODEL=openai:gpt-4o-mini

# API key for the provider (Required for cloud providers, not needed for local models like Ollama)
BUB_API_KEY=your_api_key_here

# Optional: Custom API base URL (for self-hosted or custom endpoints)
# BUB_API_BASE=https://api.custom-provider.com/v1

# Optional: Maximum tokens for AI responses (default varies by model)
# BUB_MAX_TOKENS=4096

# Optional: Custom workspace path (default: current directory)
# BUB_WORKSPACE_PATH=/path/to/your/workspace

# Optional: Custom system prompt for the AI agent
# BUB_SYSTEM_PROMPT="You are a helpful coding assistant..."

# Optional: Bub home directory (default: ~/.bub)
# BUB_HOME=/path/to/bub/home

# Example configurations for different providers:

# OpenAI GPT-4
# BUB_MODEL=openai:gpt-4
# BUB_API_KEY=sk-...

# Anthropic Claude
# BUB_MODEL=anthropic:claude-3-5-sonnet-20241022
# BUB_API_KEY=sk-ant-...

# Local Ollama (no API key needed)
# BUB_MODEL=ollama:llama3
# # BUB_API_KEY not needed

# Groq (fast inference)
# BUB_MODEL=groq:llama3-8b-8192
# BUB_API_KEY=gsk_...

# Mistral AI
# BUB_MODEL=mistral:mistral-large-latest
# BUB_API_KEY=...
