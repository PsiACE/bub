# Bub Configuration
# Copy this file to .env and update with your settings

# AI Provider Configuration (Required)
# Supported providers: openai, anthropic, ollama, groq, mistral, cohere, etc.
BUB_PROVIDER=openai

# Model name from the selected provider (Required)
# Examples:
# - OpenAI: gpt-4, gpt-3.5-turbo, gpt-4o-mini
# - Anthropic: claude-3-5-sonnet-20241022, claude-3-haiku-20240307
# - Ollama: llama2, llama3, codellama
# - Groq: llama3-8b-8192, mixtral-8x7b-32768
BUB_MODEL_NAME=gpt-3.5-turbo

# API key for the provider (Required for cloud providers, not needed for local models like Ollama)
BUB_API_KEY=your_api_key_here

# Optional: Custom API base URL (for self-hosted or custom endpoints)
# BUB_API_BASE=https://api.custom-provider.com/v1

# Optional: Maximum tokens for AI responses (default varies by model)
# BUB_MAX_TOKENS=4096

# Optional: Timeout for AI responses in seconds (default: 30)
# Increase this value if you're experiencing timeout issues with complex requests
# BUB_TIMEOUT_SECONDS=60

# Optional: Maximum number of tool execution cycles (default: 10)
# Increase this value for complex multi-step tasks, decrease for faster responses
# BUB_MAX_ITERATIONS=15

# Optional: Custom workspace path (default: current directory)
# BUB_WORKSPACE_PATH=/path/to/your/workspace

# Optional: Custom system prompt for the AI agent
# BUB_SYSTEM_PROMPT="You are a helpful coding assistant..."

# Example configurations for different providers:

# OpenAI GPT-4
# BUB_PROVIDER=openai
# BUB_MODEL_NAME=gpt-4
# BUB_API_KEY=sk-...

# Anthropic Claude
# BUB_PROVIDER=anthropic
# BUB_MODEL_NAME=claude-3-5-sonnet-20241022
# BUB_API_KEY=sk-ant-...
